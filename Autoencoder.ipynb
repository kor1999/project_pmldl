{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caroline-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "# All files are expected to be in same folder\n",
    "def parse_data(folder_path='anecdots', files_cnt=1):\n",
    "    parsed_values: list = []\n",
    "    cnt = 1\n",
    "    for each in os.listdir(folder_path):\n",
    "        with open(folder_path + '/' + each, 'r') as f:\n",
    "            buf = pd.read_csv(folder_path + '/' + each, sep=',')\n",
    "            parsed_values += buf['content'].tolist()\n",
    "        if cnt >= files_cnt:\n",
    "            break\n",
    "        else:\n",
    "            cnt += 1\n",
    "    return parsed_values\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_shape: int):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.line1 = nn.Linear(in_features=input_shape, out_features=input_shape * 3)\n",
    "        self.line2 = nn.Linear(in_features=input_shape * 3, out_features=input_shape * 9)\n",
    "        self.line3 = nn.Linear(in_features=input_shape * 9, out_features=input_shape)\n",
    "        self.line4 = nn.Linear(in_features=input_shape, out_features=input_shape // 4)\n",
    "\n",
    "        # Decoder\n",
    "        self.line5 = nn.Linear(in_features=input_shape // 4, out_features=input_shape // 2)\n",
    "        self.line6 = nn.Linear(in_features=input_shape // 2, out_features=input_shape)\n",
    "        \n",
    "        # Weight init\n",
    "        self.line1.weight.data.normal_(0.0,1/np.sqrt(input_shape))\n",
    "        self.line2.weight.data.normal_(0.0,1/np.sqrt(input_shape))\n",
    "        self.line3.weight.data.normal_(0.0,1/np.sqrt(input_shape))\n",
    "        self.line4.weight.data.normal_(0.0,1/np.sqrt(input_shape))\n",
    "        self.line5.weight.data.normal_(0.0,1/np.sqrt(input_shape))\n",
    "        self.line6.weight.data.normal_(0.0,1/np.sqrt(input_shape))\n",
    "\n",
    "    def forward(self, data: torch.Tensor):\n",
    "        z = self.encode(data)\n",
    "        z = self.decode(z)\n",
    "        return z\n",
    "\n",
    "    def encode(self, data: torch.Tensor):\n",
    "        z = F.leaky_relu(self.line1(data))\n",
    "        z = F.leaky_relu(self.line2(z))\n",
    "        z = F.leaky_relu(self.line3(z))\n",
    "        z = F.leaky_relu(self.line4(z))\n",
    "        return z\n",
    "\n",
    "    def decode(self, features: torch.Tensor):\n",
    "        z = F.relu(self.line5(features))\n",
    "        return self.line6(z)\n",
    "\n",
    "\n",
    "# data: list of words in 2d\n",
    "def idx_data(data: list):\n",
    "    lookup = sorted(list(set(itertools.chain.from_iterable([sentence_data for sentence_data in data]))))\n",
    "    lookup = {value: index for index, value in enumerate(lookup, 1)}\n",
    "    return lookup, {index: value for index, value in enumerate(lookup, 1)}\n",
    "\n",
    "\n",
    "def coalesce(*inputs):\n",
    "    for i in range(len(inputs)):\n",
    "        if inputs[i] is not None:\n",
    "            return inputs[i]\n",
    "    return 0\n",
    "\n",
    "\n",
    "# 1D list of sentences\n",
    "def preprocess(text: list) -> (torch.Tensor, dict, dict):\n",
    "    # Tokenize all sentences to words. Format is 2D: <sentence, word>\n",
    "    tokenized_dataset = list()\n",
    "    for joke in text:\n",
    "        tokenized_dataset.append(nltk.tokenize.word_tokenize(joke, language='russian'))\n",
    "\n",
    "    # Drop tail (optional)\n",
    "    res_len = [len(tokenized_dataset[i]) for i in range(len(tokenized_dataset))]\n",
    "    tokenized_dataset = tokenized_dataset[:len(tokenized_dataset) - len(tokenized_dataset) % batch_size]\n",
    "\n",
    "    # Convert tokens to vectors using Word2Vec\n",
    "    word_to_idx, ids_to_word = idx_data(tokenized_dataset)\n",
    "    indexes = []\n",
    "    for sentence in tokenized_dataset:\n",
    "        indexes.append([coalesce(word_to_idx.get(word)) for word in sentence])\n",
    "\n",
    "    # Pad to 2D matrix\n",
    "    max_line_len = len(max(tokenized_dataset, key=len))\n",
    "    tensor = torch.zeros(size=(len(text), max_line_len))\n",
    "    for i in range(len(indexes)):\n",
    "        for j in range(len(indexes[i])):\n",
    "            tensor[i, j] = indexes[i][j]\n",
    "\n",
    "    return tensor, word_to_idx, ids_to_word, res_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-playback",
   "metadata": {},
   "source": [
    "### Training\n",
    "Well, labels are sort-of \"how close are we to the source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "patient-simpson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13.144420623779297\n",
      "1 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get some data for model. We have Russian jokes.\n",
    "batch_size = 32\n",
    "rus_data = parse_data()\n",
    "\n",
    "# <cnt of lines, cnt of words>\n",
    "dataset, direct_lookup, reverse_lookup, batch_lens = preprocess(rus_data)\n",
    "# print(batch_lens)\n",
    "dataset /= len(direct_lookup)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')\n",
    "\n",
    "# Our model\n",
    "model = AE(input_shape=dataset.shape[1])\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "loss.to(device)\n",
    "\n",
    "# Train\n",
    "model.train()\n",
    "prev_sum = .0\n",
    "for epoch in range(20):\n",
    "    loss_sum = .0\n",
    "    for batch in range(len(dataset) // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(dataset[batch_size*batch:(batch+1)*batch_size, :])\n",
    "        # Generate labels {as True, False}\n",
    "        labels = torch.sign(torch.round(torch.abs(torch.sum(output - dataset[batch_size*batch:(batch+1)*batch_size, :], dim=1))))\n",
    "\n",
    "        loss_res = loss(output, labels.long())\n",
    "        loss_sum += loss_res\n",
    "        loss_res.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch, loss_sum.item())\n",
    "    if loss_sum.item() < 1e-6:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-attribute",
   "metadata": {},
   "source": [
    "### Now, let's see what we got\n",
    "Spoiler - schizophrenics will understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "stable-healthcare",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of direct lookup: 48654\n",
      ">  Читая новости, что Навальный отравился пластиковым стаканчиком, начинаешь понимать, почему их так боится Росгвардия. \n",
      " -сергей али-мпийский -зря -одних -это .потому .испанская .автор -мля -колоссально -божешь -сосед .все-таки -награждением -ихний .- .один\n",
      "\n",
      ">  Смотрел как Маск запускает космический корабль. Так и не понял куда они прячут попа. Ну не могло же оно взлететь без святой воды. \n",
      " -этому барыг -я .sb -красота 02.02.20 .через .нельзя -о -мы -на -а .пдд -первый -является .есть 0,000000000000000000000000000001\n",
      "\n",
      ">  - Ну что, отец, печенеги в городе есть?- Кому и депутат печенег. \n",
      " -нет.-вы eddisian -семилетний -у -подсудимый .- -одних -дык -дочка -девушка -почему -нет.-тогда -ихний -здравствуйте -сергей -а -со\n",
      "\n",
      ">  Если Путин знал, что Конституция такая хреновая, то почему молчал 20 лет? \n",
      " -представляете ÷ -вася -задумчивым -так .комментарий .в -ты.чё -крест-накрест -и -умирают -привет ..... -международная -вася -одних .есть\n",
      "\n",
      ">  Если вас всё устраивает в стране, значит вы – депутат Государственной Думы. \n",
      " -о instagram -страшно -хо-хо -почему .2006 -продал -зря -жуть -доктор -привет -одолжи -любой -императорский -страшно -вообще .-\n",
      "\n",
      ">  Сбер, пытаясь узнать узнать мнение клиентов о новом названии, провёл телефонный опрос и получил неожиданные результаты:10% опрошенных, после фразы \"Здравствуйте. Вас беспокоят из банка Сбер\" бросили трубку.20% послали на х*й и отключились.70% устало заявили \"Как же вы все задолбали аферисты гребаные!\" \n",
      " .впервые генассамблее .– 05/01/2018 .один 1991 160 1237-1480 -этому -так .под .германия 14-летней -г-н .⠀вместо 100лото 1917\n",
      "\n",
      ">  Девушка сказала, что я у нее первый, но шрам от кесарева как-то напрягал... \n",
      " -хватит астрономы -прибыли . -дык .я .после .задают -нет -младший -запомните -хо-хо .на -одолжи -прикинь .вечером .что\n",
      "\n",
      ">  Сказала мужу, что когда он выйдет из душа, мы поговорим о том, что я нашла у него в телефоне! Четвёртый день моется... \n",
      " -любой быстрые .джонатан .на .. 11.03.20 1-й .– -представляете -от .15 -на 0,7 -самоизоляция .дорогая .россияне 101-й\n",
      "\n",
      ">  Первая статья новой конституции России «Бог дал, Бог взял» \n",
      " -как 40 -нет.-вы -остаётся -младший -вася -ура -российским -вовочка -ах -многие -какие -сжигать -вы -нет.-тогда -почему -это\n",
      "\n",
      ">  Наташа, нет времени объяснять, снимай скорее трусы! Они могут быть отравлены! \n",
      " -перепись x -хо-хо -в -российским .вчера ..... -одних -извините -дочка -сергей -пишите -просто -колоссально -царем -зря .б\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of direct lookup: {len(direct_lookup)}')\n",
    "model.eval()\n",
    "\n",
    "# Use data\n",
    "for i in range(10):\n",
    "    testing: torch.Tensor = model(dataset[i, :])\n",
    "    # Yes, we had to use normalization in the end.\n",
    "    testing -= testing.min()\n",
    "    values: np.ndarray = np.round(testing.detach().numpy())\n",
    "    print('> ', rus_data[i], '\\n', ' '.join([reverse_lookup.get(idx) for idx in values.tolist()[:batch_lens[2]] if idx in reverse_lookup.keys()]).lower(), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "julian-henry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-перепись x -хо-хо -в -российским .вчера ..... -одних -извините -дочка -сергей -пишите -просто -колоссально -царем -зря .б .она .вечером -российским -задумчивым .она -и .2022 -божешь .8 -продал -прибыли -хватит -вот\n"
     ]
    }
   ],
   "source": [
    "# Play with size\n",
    "wanted_size = 30\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "generated = model.decode(torch.from_numpy(rng.random((1, dataset.shape[1]//4))).float())\n",
    "\n",
    "# Since we use constant-length vectors, we have to cut results\n",
    "values: np.ndarray = np.round(testing.detach().numpy())[:wanted_size]\n",
    "print(' '.join([reverse_lookup.get(idx) for idx in values.tolist() if idx in reverse_lookup.keys()]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-problem",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
