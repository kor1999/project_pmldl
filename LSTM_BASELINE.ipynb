{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "projectpmldl",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLdtiYuP6w3A"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# MODEL\n",
        "class BASELINE_simpleLSTMmodel(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(BASELINE_simpleLSTMmodel, self).__init__()\n",
        "        self.n_lstm_layers = 2\n",
        "        self.words_emb_size = 128\n",
        "        self.lstm_hidden_size = 128\n",
        "        vocabulary_length = len(dataset.words_unique)\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocabulary_length, embedding_dim=self.words_emb_size)\n",
        "        self.lstm = nn.LSTM(input_size=self.lstm_hidden_size, hidden_size=self.lstm_hidden_size, num_layers=self.n_lstm_layers)\n",
        "        self.fc = nn.Linear(self.lstm_hidden_size, vocabulary_length)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        # predict logits for next word, no softmax since CrossEntropy has it inside\n",
        "        logits = self.fc(output)\n",
        "        return logits, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUXeAzjw6w5X"
      },
      "source": [
        "# DATASET\n",
        "class NOBATCHES_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, seq_len, datapath):\n",
        "        self.seq_len = seq_len\n",
        "        self.datapath = datapath\n",
        "        train_df = pd.read_csv(self.datapath)\n",
        "        text = train_df['content'].str.cat(sep=' ')\n",
        "        self.all_words = text.split()\n",
        "        # store all words as one big list, works bad for big dataset, will make a good __getitem__ from disk next iteration\n",
        "\n",
        "        word_counts = Counter(self.all_words)\n",
        "        self.words_unique = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "        self.i2w = {index: word for index, word in enumerate(self.words_unique)}\n",
        "        self.w2i = {v:k for k,v in self.i2w.items()}\n",
        "        self.indices = [self.w2i[w] for w in self.all_words]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.indices[index:index + self.seq_len]),\n",
        "            torch.tensor(self.indices[index + 1:index + self.seq_len + 1]),\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Inz8E3vRrKE"
      },
      "source": [
        "df = pd.read_csv('all.csv')\n",
        "df2 = df.iloc[:5000]\n",
        "df2.to_csv('train.csv', index=False)\n",
        "dataset = NOBATCHES_dataset(10, 'train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbSNoTRORybH",
        "outputId": "47c730fa-7bd6-4473-afd7-874e08af1522"
      },
      "source": [
        "for x,y in dataset:\n",
        "    print('x looks like that')\n",
        "    for token in x:\n",
        "        print(dataset.i2w[int(token)])\n",
        "\n",
        "    print()\n",
        "    print('y, shifted 1 word forward, since we want to predict next word for each word in x')\n",
        "    for token in y:\n",
        "        print(dataset.i2w[int(token)])\n",
        "    break"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x looks like that\n",
            "Недорого\n",
            "окультуриваем\n",
            "прямо\n",
            "из\n",
            "Санкт-Петербурга!Прививаем\n",
            "любовь\n",
            "к\n",
            "театру,\n",
            "формируем\n",
            "литературную\n",
            "\n",
            "y, shifted 1 word forward, since we want to predict next word for each word in x\n",
            "окультуриваем\n",
            "прямо\n",
            "из\n",
            "Санкт-Петербурга!Прививаем\n",
            "любовь\n",
            "к\n",
            "театру,\n",
            "формируем\n",
            "литературную\n",
            "зависимость,подсаживаем\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3x8Ms5qSS8y"
      },
      "source": [
        "# it can be clearly seen that data is splitted bad for tokens, so we will fix it in next iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DDacV4pRVsp"
      },
      "source": [
        "batch_size = 200\n",
        "max_epochs = 10\n",
        "seq_len = 10\n",
        "datapath = 'train.csv'  # try to use small file first\n",
        "\n",
        "dataset = NOBATCHES_dataset(seq_len=seq_len, datapath=datapath)\n",
        "model = BASELINE_simpleLSTMmodel(dataset)\n",
        "\n",
        "model.train()\n",
        "model = model.cuda()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "    # init states to zero\n",
        "    state_h = torch.zeros(model.n_lstm_layers, seq_len, model.lstm_hidden_size)\n",
        "    state_c = torch.zeros(model.n_lstm_layers, seq_len, model.lstm_hidden_size)\n",
        "\n",
        "    for batch, (x, y) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred, (state_h, state_c) = model(x.cuda(), (state_h.cuda(), state_c.cuda()))\n",
        "        loss = criterion(y_pred.transpose(1, 2), y.cuda())\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print({'loss': loss.item()})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z96l8bGhRVu6",
        "outputId": "f57e4b36-e2aa-4e6e-c930-dae803b31568"
      },
      "source": [
        "# TRY TO generate some anecdotes\n",
        "model.eval()\n",
        "model = model.cpu()\n",
        "\n",
        "\n",
        "def gen_anec(condition_text, n_trials, next_words):\n",
        "    for trial in range(n_trials):\n",
        "        words = condition_text.split(' ')\n",
        "        state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "        for i in range(0, next_words):\n",
        "            x = torch.tensor([[dataset.w2i[w] for w in words[i:]]])\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "            last_word_logits = y_pred[0][-1]\n",
        "            softmaxed = torch.nn.functional.softmax(last_word_logits, dim=0).detach()\n",
        "\n",
        "            ## uncomment for greedy word selection\n",
        "            # maxind = torch.argmax(softmaxed)\n",
        "            # words.append(dataset.index_to_word[int(maxind.detach())])\n",
        "\n",
        "            ## uncomment for probabilistic selection of next word\n",
        "            word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "            words.append(dataset.i2w[word_index])\n",
        "\n",
        "        print(' '.join(words))\n",
        "\n",
        "\n",
        "gen_anec('Заходит как то', 5, 10)\n",
        "gen_anec('А что это вы', 5, 10)\n",
        "gen_anec('Водка', 5, 10)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Заходит как то России Забегает полицейского пули шкафу прямоготелефона интрига протираются, того Кстати,\n",
            "Заходит как то пробками! свободу ним обязаностям пиромана, было найдёте завод нашем нему\n",
            "Заходит как то тридцать этом том,  стрелять забиваешь Оксана поддержать, нашей такой\n",
            "Заходит как то относятся лес, месте. Г. вот!Кардиолог 4 того думал... мире прямоготелефона\n",
            "Заходит как то Хорошо, золотую несколько страшный ведёрком подарить вас миллиардов некоторых Метро!-\n",
            "А что это вы магазин скаку чемпионами Дмитрия России олимпийцы того,  смысле кухне\n",
            "А что это вы Чмаровка, последний отличие замене морякирасстреляли здоровье! имени надпись К. Михаилу\n",
            "А что это вы лосей Вовочка Москве Так, скамью пузо одном этом работе. инциндента\n",
            "А что это вы 2 отсутствием Хабаровска блюдо, него Москве, жизнь сбываться.- примером было,\n",
            "А что это вы \"пенис\" оказался!!!! тобой бегемота  том надзирателем. по-пластунски контрабандным, точно\n",
            "Водка Тверской 60 пятеро ГИБДД года ушами\" России левом Скрестили футболу\n",
            "Водка  борьбе ней своей курить поводу праздник? госучреждений президента...Так ним.И\n",
            "Водка него суда помощью Питере 90нанометров, плавать Собчак Пока флешкой наеба\".\n",
            "Водка России такси чиновницами, 65 русском вышел. инциндента телевизору бюджет Обед.\n",
            "Водка мозг. предложению получил 10 главной называется. месяц сейчас, продажи, Украине\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KZGbEo5RVzd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeG2qXxyRV1o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYd0-XT8RV4a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}